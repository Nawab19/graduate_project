{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from functools import partial\n",
    "from modAL.batch import uncertainty_batch_sampling, ranked_batch\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from numpy import quantile, where, random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib as mpl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"movie_metadata.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop_duplicates()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"director_name\",\"actor_2_name\",\"genres\",\"movie_title\",\"actor_1_name\",\"actor_3_name\",\"language\",\"country\",\"plot_keywords\",\"movie_imdb_link\",\"cast_total_facebook_likes\"],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLING NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"gross\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Target Variables for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting IMDB SCORE into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"imdb_score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"imdb_score\"] = data[\"imdb_score\"].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['imdb_score'].between(8,10), 'imdb_score'] = 100.0\n",
    "data.loc[data['imdb_score'].between(5,7.99), 'imdb_score'] = 50.0\n",
    "data.loc[data['imdb_score'].between(0,4.992), 'imdb_score'] = 30.0\n",
    "data[\"imdb_score\"] = data[\"imdb_score\"].apply(str)\n",
    "data.loc[data['imdb_score'] == \"100.0\", 'imdb_score'] = \"BEST\"\n",
    "data.loc[data['imdb_score'] == \"50.0\", 'imdb_score'] = \"AVERAGE\"\n",
    "data.loc[data['imdb_score'] == \"30.0\", 'imdb_score'] = \"BAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"imdb_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Content Ratings into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data[\"content_rating\"].unique()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate in ratings:\n",
    "    if rate == \"M\":\n",
    "        data.loc[data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        #print(\"PG-13\")\n",
    "    elif rate == \"GP\":\n",
    "        data.loc[data['content_rating'] == rate, 'content_rating'] = \"PG\"\n",
    "        #print(\"Others\")\n",
    "    elif rate == \"Unrated\":\n",
    "        data.loc[data['content_rating'] == rate, 'content_rating'] = \"Not Rated\"\n",
    "        #print(\"Others\")\n",
    "    elif rate == \"Passed\":\n",
    "        data.loc[data['content_rating'] == rate, 'content_rating'] = \"Approved\"\n",
    "        #print(\"Others\")\n",
    "    elif rate == \"X\":\n",
    "        data.loc[data['content_rating'] == rate, 'content_rating'] = \"NC-17\"\n",
    "        #print(\"Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting gross into 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"gross\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['gross'].between(0,30000000.0), 'gross'] = 0.0\n",
    "data.loc[data['gross'].between(3000000.01,762000000.0), 'gross'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"gross\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['gross'] == 100.0, 'gross'] = \"Above Average\"\n",
    "#data.loc[data['gross'] == 30.0, 'gross'] = \"Below Average\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"gross\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "target1 = le.fit(data[\"imdb_score\"]).transform(data[\"imdb_score\"])\n",
    "target2 = le.fit(data[\"content_rating\"]).transform(data[\"content_rating\"])\n",
    "target3 = le.fit(data[\"gross\"]).transform(data[\"gross\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = data.drop([\"imdb_score\", \"content_rating\", \"gross\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(performance,type = 'Accuracy'):\n",
    "    # Plot our performance over time.\n",
    "    fig, ax = plt.subplots(figsize=(5, 3), dpi=130)\n",
    "  \n",
    "    ax.plot(performance)\n",
    "    ax.scatter(range(len(performance)), performance, s=13)\n",
    "\n",
    "    ax.set_title('Incremental classification '+str(type))\n",
    "    ax.set_xlabel('Query iteration')\n",
    "    ax.set_ylabel('Classification '+str(type))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_samples(X_raw_a, y_raw_1_a, y_raw_2_a, y_raw_3_a):\n",
    "    \n",
    "    train_instances = []\n",
    "    train_instances_y1 = []\n",
    "    train_instances_y2 = []\n",
    "    train_instances_y3 = []\n",
    "    \n",
    "    #stratifying target 3\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    \n",
    "    counter_inst = 0\n",
    "    for q in range(len(y_raw_3_a)):\n",
    "        if ((y_raw_3_a[q-counter_inst]) == 1) and (count_b<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_b = count_b + 1\n",
    "            \n",
    "        elif ((y_raw_3_a[q-counter_inst]) == 0) and (count_a<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_a = count_a + 1\n",
    "    \n",
    "    #stratifying target 2\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    count_c = 0\n",
    "    count_d = 0\n",
    "    count_e = 0\n",
    "    count_f = 0\n",
    "    count_g = 0\n",
    "    \n",
    "    counter_inst = 0\n",
    "    for q in range(len(y_raw_2_a)):\n",
    "        if ((y_raw_2_a[q-counter_inst]) == 0) and (count_a<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_a = count_a + 1\n",
    "            \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 1) and (count_b<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_b = count_b + 1\n",
    "            \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 2) and (count_c<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_c = count_c + 1\n",
    "        \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 3) and (count_d<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_d = count_d + 1\n",
    "        \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 4) and (count_e<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_e = count_e + 1\n",
    "            \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 5) and (count_f<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_f = count_f + 1\n",
    "        \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 6) and (count_g<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_g = count_g + 1\n",
    "            \n",
    "    #stratifying target 1\n",
    "    count_a = 0\n",
    "    count_b = 0\n",
    "    count_c = 0\n",
    "    \n",
    "    counter_inst = 0\n",
    "    for q in range(len(y_raw_1_a)):\n",
    "        if ((y_raw_2_a[q-counter_inst]) == 0) and (count_a<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_a = count_a + 1\n",
    "            \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 1) and (count_b<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_b = count_b + 1\n",
    "            \n",
    "        elif ((y_raw_2_a[q-counter_inst]) == 2) and (count_c<10):\n",
    "            train_instances.append(X_raw_a[q-counter_inst])\n",
    "            train_instances_y1.append(y_raw_1_a[q-counter_inst])\n",
    "            train_instances_y2.append(y_raw_2_a[q-counter_inst])\n",
    "            train_instances_y3.append(y_raw_3_a[q-counter_inst])\n",
    "            \n",
    "            X_raw_a = np.delete(X_raw_a,q-counter_inst,axis=0)\n",
    "            y_raw_1_a = np.delete(y_raw_1_a,q-counter_inst,axis=0)\n",
    "            y_raw_2_a = np.delete(y_raw_2_a,q-counter_inst,axis=0)\n",
    "            y_raw_3_a = np.delete(y_raw_3_a,q-counter_inst,axis=0)\n",
    "            counter_inst = counter_inst + 1\n",
    "            count_c = count_c + 1\n",
    "    \n",
    "    return train_instances, len(train_instances), train_instances_y1, train_instances_y2, train_instances_y3, X_raw_a, y_raw_1_a, y_raw_2_a, y_raw_3_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVE LEARNING TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring batch size and initial raw samples\n",
    "BATCH_SIZE = 50\n",
    "N_RAW_SAMPLES= 1000\n",
    "\n",
    "#Declaring Features and Target variables\n",
    "y_raw_1 = target1\n",
    "y_raw_2 = target2\n",
    "y_raw_3 = target3\n",
    "X_raw = X_final\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Shape of data set :\" + str(X_raw.shape))\n",
    "\n",
    "# converting data from dataframe to ndarray\n",
    "#y_raw_1 = y_raw_1.values\n",
    "#y_raw_2 = y_raw_2.values\n",
    "#y_raw_3 = y_raw_3.values\n",
    "X_raw = X_raw.values\n",
    "\n",
    "# batch_data_continous_X,y stores data and adds to it after every iteration of batch\n",
    "batch_data_continous_X = np.array([]).reshape(0,X_raw.shape[1])\n",
    "batch_data_continous_y_1 = np.array([]).reshape(0,1)\n",
    "batch_data_continous_y_2 = np.array([]).reshape(0,1)\n",
    "batch_data_continous_y_3 = np.array([]).reshape(0,1)\n",
    "\n",
    "# Spliting dataset into Train, Pool(for batch) and Test.\n",
    "print(\"Preparing Initial Training, Pool and Test sets for initial AL\")\n",
    "print(\"\")\n",
    "\n",
    "X_train, length_train, y_train_1, y_train_2, y_train_3, X_raw, y_raw_1, y_raw_2, y_raw_3 = stratify_samples(X_raw, y_raw_1, y_raw_2, y_raw_3)\n",
    "X_train = np.array(X_train)\n",
    "y_train_1 = np.array(y_train_1)\n",
    "y_train_2 = np.array(y_train_2)\n",
    "y_train_3 = np.array(y_train_3)\n",
    "\n",
    "X_train_temp = X_raw[:884]\n",
    "y_train_1_temp = y_raw_1[:884]\n",
    "y_train_2_temp = y_raw_2[:884]\n",
    "y_train_3_temp = y_raw_3[:884]\n",
    "\n",
    "#Joining Training Set\n",
    "X_train = np.concatenate((X_train, X_train_temp), axis=0)\n",
    "y_train_1 = np.concatenate((y_train_1, y_train_1_temp), axis=0) \n",
    "y_train_2 = np.concatenate((y_train_2, y_train_2_temp), axis=0) \n",
    "y_train_3 = np.concatenate((y_train_3, y_train_3_temp), axis=0) \n",
    "\n",
    "X_pool = X_raw[884:2884]\n",
    "y_pool_1 = y_raw_1[884:2884]\n",
    "y_pool_2 = y_raw_2[884:2884]\n",
    "y_pool_3 = y_raw_3[884:2884]\n",
    "\n",
    "X_test = X_raw[2884:]\n",
    "y_test_1 = y_raw_1[2884:]\n",
    "y_test_2 = y_raw_2[2884:]\n",
    "y_test_3 = y_raw_3[2884:]\n",
    "\n",
    "print(\"Training set size: \")\n",
    "print(\"---------------------------\")\n",
    "print(\"X_train: \"+str(len(X_train)))\n",
    "print(\"Y_train_1: \"+str(len(y_train_1)))\n",
    "print(\"Y_train_2: \"+str(len(y_train_2)))\n",
    "print(\"Y_train_3: \"+str(len(y_train_3)))\n",
    "print(\"\")\n",
    "print(\"Pool set size: \")\n",
    "print(\"---------------------------\")\n",
    "print(\"X_pool: \"+str(len(X_pool)))\n",
    "print(\"Y_pool_1: \"+str(len(y_pool_1)))\n",
    "print(\"Y_pool_2: \"+str(len(y_pool_2)))\n",
    "print(\"Y_pool_3: \"+str(len(y_pool_3)))\n",
    "print(\"\")\n",
    "print(\"Test set size: \")\n",
    "print(\"---------------------------\")\n",
    "print(\"X_test: \"+str(len(X_test)))\n",
    "print(\"Y_test_1: \"+str(len(y_test_1)))\n",
    "print(\"Y_test_2: \"+str(len(y_test_2)))\n",
    "print(\"Y_test_3: \"+str(len(y_test_3)))\n",
    "print(\"\")\n",
    "\n",
    "# lists to store all metric values after each iteration\n",
    "accuracy_history = {1:[], 2:[], 3:[]}\n",
    "auc_history = {1:[], 2:[], 3:[]}\n",
    "precision_history = {1:[], 2:[], 3:[]}\n",
    "recall_history = {1:[], 2:[], 3:[]}\n",
    "f1_history = {1:[], 2:[], 3:[]}\n",
    "clf_1 = DecisionTreeClassifier(criterion= 'gini', max_depth= 7, splitter= 'best')\n",
    "clf_2 = DecisionTreeClassifier(criterion= 'gini', max_depth= 7, splitter= 'best')\n",
    "clf_3 = DecisionTreeClassifier(criterion= 'gini', max_depth= 7, splitter= 'best')\n",
    "\n",
    "batch_data_continous_X = np.vstack([batch_data_continous_X, X_train])\n",
    "batch_data_continous_y_1 = np.append(batch_data_continous_y_1, y_train_1)\n",
    "batch_data_continous_y_2 = np.append(batch_data_continous_y_2, y_train_2)\n",
    "batch_data_continous_y_3 = np.append(batch_data_continous_y_3, y_train_3)\n",
    "\n",
    "# Initial training with trainset\n",
    "print(\"========================\")\n",
    "print(\"Initial Training Results\")\n",
    "\n",
    "#Fitting the model for each label\n",
    "clf1 = clf_1.fit(X_train, y_train_1)\n",
    "clf2 = clf_2.fit(X_train, y_train_2)\n",
    "clf3 = clf_3.fit(X_train, y_train_3)\n",
    "\n",
    "#Generating predictions for each label\n",
    "predictions_1 = clf1.predict(X_test)\n",
    "predictions_2 = clf2.predict(X_test)\n",
    "predictions_3 = clf3.predict(X_test)\n",
    "\n",
    "#Generating accuracies for each label\n",
    "model_accuracy_1 = accuracy_score(y_test_1,predictions_1)\n",
    "model_accuracy_2 = accuracy_score(y_test_2,predictions_2)\n",
    "model_accuracy_3 = accuracy_score(y_test_3,predictions_3)\n",
    "\n",
    "print(\"========================\")\n",
    "print('Accuracy after Query {n} for Target 1: {acc:0.4f}'.format(n=0, acc=model_accuracy_1))\n",
    "print('Accuracy after Query {n} for Target 2: {acc:0.4f}'.format(n=0, acc=model_accuracy_2))\n",
    "print('Accuracy after Query {n} for Target 3: {acc:0.4f}'.format(n=0, acc=model_accuracy_3))\n",
    "\n",
    "#Appending accuracy score for all labels\n",
    "accuracy_history[1].append(model_accuracy_1)\n",
    "accuracy_history[2].append(model_accuracy_2)\n",
    "accuracy_history[3].append(model_accuracy_3)\n",
    "\n",
    "#Appending auc score for all labels\n",
    "#auc_history[1].append(roc_auc_score(y_test_1, predictions_1, multi_class=\"ovr\"))\n",
    "#auc_history[2].append(roc_auc_score(y_test_2, predictions_2, multi_class=\"ovr\"))\n",
    "#auc_history[3].append(roc_auc_score(y_test_3, predictions_3, multi_class=\"ovr\"))\n",
    "\n",
    "#Appending f1 score for all labels\n",
    "f1_history[1].append(f1_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "f1_history[2].append(f1_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "f1_history[3].append(f1_score(y_test_3, predictions_3, average=\"macro\"))\n",
    "\n",
    "#Appending recall for all labels\n",
    "recall_history[1].append(recall_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "recall_history[2].append(recall_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "recall_history[3].append(recall_score(y_test_3, predictions_3, average=\"macro\"))\n",
    "\n",
    "#Appending precision score for all labels\n",
    "precision_history[1].append(precision_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "precision_history[2].append(precision_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "precision_history[3].append(precision_score(y_test_3, predictions_3, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_probabilities(target_prob):\n",
    "    prob_arr_temp = [-1,-1,-1,-1,-1,-1,-1]\n",
    "    prob_array = []\n",
    "    for i in range(target_prob.shape[0]):\n",
    "        for j in range(target_prob.shape[1]):\n",
    "            prob_arr_temp[j] = target_prob[i][j]\n",
    "        max = np.amax(prob_arr_temp)\n",
    "        class_index = np.where(prob_arr_temp == max)\n",
    "        prob_array.append([max,class_index[0][0],i])\n",
    "        #print([max,class_index[0][0],i])\n",
    "    return prob_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min(tar_probs):\n",
    "    temp_tar_probs = tar_probs\n",
    "    que_instances = []\n",
    "    temp_tar_probs = sorted(temp_tar_probs, key=lambda x:x[0])\n",
    "    count_stop = 1\n",
    "    for t in range(50):\n",
    "        if ((count_stop < 16) and (int(temp_tar_probs[t][2]) < len(temp_tar_probs))):\n",
    "            que_instances.append(int(temp_tar_probs[t][2]))\n",
    "            count_stop = count_stop + 1\n",
    "            \n",
    "    return que_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining Query Samples\n",
    "def obt_samples(tar_probs, pool_x, pool_y_1, pool_y_2, pool_y_3, query_indexs, query_samples_xs, query_samples_y_1s, query_samples_y_2s, query_samples_y_3s, tar_probs_2, tar_probs_3):\n",
    "    q_indic = find_min(tar_probs)\n",
    "    q_indic = sorted(q_indic, reverse=True)\n",
    "    for p in range(len(q_indic)):\n",
    "        q_ind = q_indic[p]\n",
    "        query_indexs.append(q_ind)\n",
    "\n",
    "        query_samples_xs.append(pool_x[q_ind])\n",
    "        query_samples_y_1s.append(pool_y_1[q_ind])\n",
    "        query_samples_y_2s.append(pool_y_2[q_ind])\n",
    "        query_samples_y_3s.append(pool_y_3[q_ind])\n",
    "        \n",
    "    for p in range(len(q_indic)):\n",
    "        q_ind = q_indic[p]\n",
    "        pool_x = np.delete(pool_x,q_ind,axis=0)\n",
    "        pool_y_1 = np.delete(pool_y_1,q_ind,axis=0)\n",
    "        pool_y_2 = np.delete(pool_y_2,q_ind,axis=0)\n",
    "        pool_y_3 = np.delete(pool_y_3,q_ind,axis=0)\n",
    "        tar_probs = np.delete(tar_probs,q_ind,axis=0)\n",
    "        tar_probs_2 = np.delete(tar_probs_2,q_ind,axis=0)\n",
    "        tar_probs_3 = np.delete(tar_probs_3,q_ind,axis=0)\n",
    "        \n",
    "    return pool_x, pool_y_1, pool_y_2, pool_y_3, tar_probs, query_indexs, query_samples_xs, query_samples_y_1s, query_samples_y_2s, query_samples_y_3s, tar_probs_2, tar_probs_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_samples(X_pools, Y_pools_1, Y_pools_2, Y_pools_3, tar_1_proba, tar_2_proba, tar_3_proba):\n",
    "    tar_1_high_probs = high_probabilities(tar_1_proba)\n",
    "    tar_2_high_probs = high_probabilities(tar_2_proba)\n",
    "    tar_3_high_probs = high_probabilities(tar_3_proba)\n",
    "    \n",
    "    query_index = []\n",
    "    query_samples_x = []\n",
    "    query_samples_y_1 = []\n",
    "    query_samples_y_2 = []\n",
    "    query_samples_y_3 = []\n",
    "    \n",
    "    print(\"X_pool length before\",len(X_pool))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Y_pools_1 length before\",len(Y_pools_1))\n",
    "    print(\"Y_pools_2 length before\",len(Y_pools_2))\n",
    "    print(\"Y_pools_3 length before\",len(Y_pools_3))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"tar_1_high_probs length before\",len(tar_1_high_probs))\n",
    "    print(\"tar_2_high_probs length before\",len(tar_2_high_probs))\n",
    "    print(\"tar_3_high_probs length before\",len(tar_3_high_probs))\n",
    "    print()\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    X_pools, Y_pools_1, Y_pools_2, Y_pools_3, tar_1_high_probs, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_2_high_probs, tar_3_high_probs = obt_samples(tar_1_high_probs, X_pools, Y_pools_1, Y_pools_2, Y_pools_3, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_2_high_probs, tar_3_high_probs)\n",
    "    X_pools, Y_pools_1, Y_pools_2, Y_pools_3, tar_2_high_probs, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_1_high_probs, tar_3_high_probs = obt_samples(tar_2_high_probs, X_pools, Y_pools_1, Y_pools_2, Y_pools_3, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_1_high_probs, tar_3_high_probs)\n",
    "    X_pools, Y_pools_1, Y_pools_2, Y_pools_3, tar_3_high_probs, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_1_high_probs, tar_2_high_probs = obt_samples(tar_3_high_probs, X_pools, Y_pools_1, Y_pools_2, Y_pools_3, query_index, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3, tar_1_high_probs, tar_2_high_probs)\n",
    "    \n",
    "    print(\"X_pool length after\",len(X_pools))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Y_pools_1 length after\",len(Y_pools_1))\n",
    "    print(\"Y_pools_2 length after\",len(Y_pools_2))\n",
    "    print(\"Y_pools_3 length after\",len(Y_pools_3))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"tar_1_high_probs length after\",len(tar_1_high_probs))\n",
    "    print(\"tar_2_high_probs length after\",len(tar_2_high_probs))\n",
    "    print(\"tar_3_high_probs length after\",len(tar_3_high_probs))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"Query_Index length\",len(query_index))\n",
    "    print(\"Query_Samples_Y1 length\",len(query_samples_y_1))\n",
    "    print(\"Query_Samples_Y2 length\",len(query_samples_y_2))\n",
    "    print(\"Query_Samples_Y3 length\",len(query_samples_y_3))\n",
    "    \n",
    "    return X_pools, Y_pools_1, Y_pools_2, Y_pools_3, query_samples_x, query_samples_y_1, query_samples_y_2, query_samples_y_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_QUERIES = int(N_RAW_SAMPLES // BATCH_SIZE)\n",
    "N_QUERIES = 20\n",
    "print(\"========================\")\n",
    "for index in range(N_QUERIES):\n",
    "    \n",
    "    tar_1_probas = clf1.predict_proba(X_pool)\n",
    "    tar_2_probas = clf2.predict_proba(X_pool)\n",
    "    tar_3_probas = clf3.predict_proba(X_pool)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"Learning phase Query: \" + str(index + 1))\n",
    "\n",
    "    print(\"Getting pool of data for learner\")\n",
    "      \n",
    "    #query_index, query_instance = learner.query(X_pool)\n",
    "    X_pool, y_pool_1, y_pool_2, y_pool_3, X_batch, y_batch_1, y_batch_2, y_batch_3 = find_samples(X_pool, y_pool_1, y_pool_2, y_pool_3, tar_1_probas, tar_2_probas, tar_3_probas)\n",
    "    \n",
    "    batch_data_continous_X = np.vstack([batch_data_continous_X, X_batch])\n",
    "    batch_data_continous_y_1 = np.append(batch_data_continous_y_1, y_batch_1)\n",
    "    batch_data_continous_y_2 = np.append(batch_data_continous_y_2, y_batch_2)\n",
    "    batch_data_continous_y_3 = np.append(batch_data_continous_y_3, y_batch_3)\n",
    "    \n",
    "    #Fitting the model for each label\n",
    "    clf1 = clf_1.fit(batch_data_continous_X, batch_data_continous_y_1)\n",
    "    clf2 = clf_2.fit(batch_data_continous_X, batch_data_continous_y_2)\n",
    "    clf3 = clf_3.fit(batch_data_continous_X, batch_data_continous_y_3)\n",
    "\n",
    "    #Generating predictions for each label\n",
    "    predictions_1 = clf1.predict(X_test)\n",
    "    predictions_2 = clf2.predict(X_test)\n",
    "    predictions_3 = clf3.predict(X_test)\n",
    "\n",
    "    #Generating accuracies for each label\n",
    "    model_accuracy_1 = accuracy_score(y_test_1,predictions_1)\n",
    "    model_accuracy_2 = accuracy_score(y_test_2,predictions_2)\n",
    "    model_accuracy_3 = accuracy_score(y_test_3,predictions_3)\n",
    "\n",
    "    print(\"========================\")\n",
    "    print('Accuracy after Query {n} for Target 1: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy_1))\n",
    "    print('Accuracy after Query {n} for Target 2: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy_2))\n",
    "    print('Accuracy after Query {n} for Target 3: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy_3))\n",
    "\n",
    "    #Appending accuracy score for all labels\n",
    "    accuracy_history[1].append(model_accuracy_1)\n",
    "    accuracy_history[2].append(model_accuracy_2)\n",
    "    accuracy_history[3].append(model_accuracy_3)\n",
    "\n",
    "    #Appending auc score for all labels\n",
    "    #auc_history[1].append(roc_auc_score(y_test_1, predictions_1, multi_class=\"ovr\"))\n",
    "    #auc_history[2].append(roc_auc_score(y_test_2, predictions_2, multi_class=\"ovr\"))\n",
    "    #auc_history[3].append(roc_auc_score(y_test_3, predictions_3, multi_class=\"ovr\"))\n",
    "\n",
    "    #Appending f1 score for all labels\n",
    "    f1_history[1].append(f1_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "    f1_history[2].append(f1_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "    f1_history[3].append(f1_score(y_test_3, predictions_3, average=\"macro\"))\n",
    "\n",
    "    #Appending recall for all labels\n",
    "    recall_history[1].append(recall_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "    recall_history[2].append(recall_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "    recall_history[3].append(recall_score(y_test_3, predictions_3, average=\"macro\"))\n",
    "\n",
    "    #Appending precision score for all labels\n",
    "    precision_history[1].append(precision_score(y_test_1, predictions_1, average=\"macro\"))\n",
    "    precision_history[2].append(precision_score(y_test_2, predictions_2, average=\"macro\"))\n",
    "    precision_history[3].append(precision_score(y_test_3, predictions_3, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plotGraph(accuracy_history[1],'Accuracy for Target 1')\n",
    "plotGraph(accuracy_history[2],'Accuracy for Target 2')\n",
    "plotGraph(accuracy_history[3],'Accuracy for Target 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plotGraph(f1_history[1],'F1 Score for Target 1')\n",
    "plotGraph(f1_history[2],'F1 Score for Target 2')\n",
    "plotGraph(f1_history[3],'F1 Score for Target 3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
